{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77cfd0f3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d2908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14405db1",
   "metadata": {},
   "source": [
    "# Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8249bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_paths(data_folder=None, date=None):\n",
    "    if data_folder:\n",
    "        shapes_df_path = os.path.join(data_folder, 'shapes.csv')\n",
    "        routes_df_path = os.path.join(data_folder, 'routes.csv')\n",
    "        route_versions_df_path = os.path.join(data_folder, 'route_versions.csv')\n",
    "        shape_variants_df_path = os.path.join(data_folder, 'shape_variants.csv')\n",
    "        shape_variant_activations_df_path = os.path.join(data_folder, 'shape_variant_activations.csv')\n",
    "\n",
    "        temporary_changes_df_path = os.path.join(data_folder, 'temporary_changes.csv')\n",
    "        return shapes_df_path, routes_df_path, route_versions_df_path, shape_variants_df_path, shape_variant_activations_df_path, temporary_changes_df_path\n",
    "\n",
    "    if date:\n",
    "        date_folder = '../data/raw/'\n",
    "        routes_path = os.path.join(date_folder, date, 'routes.txt')\n",
    "        trips_path = os.path.join(date_folder, date, 'trips.txt')\n",
    "        shapes_path = os.path.join(date_folder, date, 'shapes.txt')\n",
    "        calendar_path = os.path.join(date_folder, date, 'calendar.txt')\n",
    "        calendar_dates_path = os.path.join(date_folder, date, 'calendar_dates.txt')\n",
    "\n",
    "        return routes_path, trips_path, shapes_path, calendar_path, calendar_dates_path\n",
    "\n",
    "    raise ValueError(\"Either data_folder or date must be provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d02ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_txt_data(date, print_shapes=False):\n",
    "    routes_path, trips_path, shapes_path, calendar_path, calendar_dates_path = define_paths(date=date)\n",
    "    routes_txt = pd.read_csv(routes_path)\n",
    "    trips_txt = pd.read_csv(trips_path)\n",
    "    shapes_txt = pd.read_csv(shapes_path)\n",
    "    calendar_dates_txt = pd.read_csv(calendar_dates_path)\n",
    "\n",
    "    if print_shapes:\n",
    "        print(\"Routes:\", routes_txt.shape)\n",
    "        print(\"Trips:\", trips_txt.shape)\n",
    "        print(\"Shapes:\", shapes_txt.shape)\n",
    "        print(\"Calendar Dates:\", calendar_dates_txt.shape)\n",
    "\n",
    "    try:  # Check if the file exists\n",
    "        calendar_txt = pd.read_csv(calendar_path, parse_dates=['start_date', 'end_date'])\n",
    "    except FileNotFoundError:\n",
    "        # Make empty dataframes for the first time\n",
    "        print(\"Calendar file not found. Creating empty dataframe.\")\n",
    "        calendar_txt = pd.DataFrame(columns=['service_id', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'start_date', 'end_date'])\n",
    "        calendar_txt['start_date'] = pd.to_datetime(calendar_txt['start_date'])\n",
    "        calendar_txt['end_date'] = pd.to_datetime(calendar_txt['end_date'])\n",
    "    \n",
    "    return routes_txt, trips_txt, shapes_txt, calendar_txt, calendar_dates_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd042332",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '20131018'\n",
    "routes_txt, trips_txt, shapes_txt, calendar_txt, calendar_dates_txt = load_txt_data(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1849da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_data(data_folder):\n",
    "    shapes_df_path, routes_df_path, route_versions_df_path, shape_variants_df_path, shape_variant_activations_df_path, temporary_changes_df_path = define_paths(data_folder=data_folder)\n",
    "\n",
    "    try:\n",
    "        shapes_df = pd.read_csv(shapes_df_path)\n",
    "        routes_df = pd.read_csv(routes_df_path)\n",
    "        route_versions_df = pd.read_csv(route_versions_df_path, parse_dates=['valid_from', 'valid_to'])\n",
    "        shape_variants_df = pd.read_csv(shape_variants_df_path)\n",
    "        shape_variant_activations_df = pd.read_csv(shape_variant_activations_df_path)\n",
    "        temporary_changes_df = pd.read_csv(temporary_changes_df_path)\n",
    "    except FileNotFoundError:\n",
    "        # Make empty dataframes for the first time\n",
    "        ### shapes_df ###\n",
    "        shapes_df = pd.DataFrame(columns=[\n",
    "            \"shape_id\", \"shape_pt_lat\", \"shape_pt_lon\", \"shape_pt_sequence\", \"shape_dist_traveled\", \"shape_bkk_ref\"\n",
    "        ])\n",
    "\n",
    "        ### routes_df ###\n",
    "        routes_df = pd.DataFrame(columns=[\n",
    "            \"route_id\", \"agency_id\", \"route_short_name\", \"route_type\", \"route_color\", \"route_text_color\"\n",
    "        ])\n",
    "\n",
    "        ### route_versions_df ###\n",
    "        route_versions_df = pd.DataFrame(columns=[\n",
    "            \"version_id\", \"route_id\", \"direction_id\", \"route_long_name\", \"route_desc\",\n",
    "            \"valid_from\", \"valid_to\", \"main_shape_id\", \"trip_headsign\",\n",
    "            \"parent_version_id\", \"note\"\n",
    "        ])\n",
    "        # valid_from and valid_to be converted to datetime\n",
    "        route_versions_df['valid_from'] = pd.to_datetime(route_versions_df['valid_from'])\n",
    "        route_versions_df['valid_to'] = pd.to_datetime(route_versions_df['valid_to'])\n",
    "        \n",
    "        ### shape_variants_df ###\n",
    "        shape_variants_df = pd.DataFrame(columns=[\n",
    "            \"shape_variant_id\", \"version_id\", \"shape_id\", \"trip_headsign\", \"is_main\", \"note\"\n",
    "        ])\n",
    "\n",
    "        ### shape_variant_activations_df ###\n",
    "        shape_variant_activations_df = pd.DataFrame(columns=[\n",
    "            \"date\", \"shape_variant_id\", \"exception_type\"\n",
    "        ])\n",
    "        #shape_variant_activations_df[\"exception_type\"]\n",
    "        shape_variant_activations_df.astype({\"exception_type\": \"float64\"})\n",
    "\n",
    "        ### temporary_changes_df ###\n",
    "        temporary_changes_df = pd.DataFrame(columns=[\n",
    "            \"detour_id\", \"route_id\", \"start_date\", \"end_date\", \"affects_version_id\", \"description\"\n",
    "        ])\n",
    "        # Save\n",
    "        shapes_df.to_csv(shapes_df_path, index=False)\n",
    "        routes_df.to_csv(routes_df_path, index=False)\n",
    "        route_versions_df.to_csv(route_versions_df_path, index=False)\n",
    "        shape_variants_df.to_csv(shape_variants_df_path, index=False)\n",
    "        shape_variant_activations_df.to_csv(shape_variant_activations_df_path, index=False)\n",
    "        temporary_changes_df.to_csv(temporary_changes_df_path, index=False)\n",
    "\n",
    "    return shapes_df, routes_df, route_versions_df, shape_variants_df, shape_variant_activations_df, temporary_changes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc566d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/processed/'\n",
    "shapes_df, routes_df, route_versions_df, shape_variants_df, shape_variant_activations_df, temporary_changes_df = load_df_data(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb69a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = trips_txt.copy()\n",
    "calendar_df = calendar_txt.copy()\n",
    "calendar_dates_df = calendar_dates_txt.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed62a6",
   "metadata": {},
   "source": [
    "## Info from calendar_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "207cd426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_dates(df, service_id, to_string=True, date_format='%Y-%m-%d'):\n",
    "    \"\"\"\n",
    "    Get all active dates for a service.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with service data\n",
    "        service_id: ID of the service to look up\n",
    "        to_string: Convert dates to strings\n",
    "        date_format: Format for string conversion\n",
    "    \n",
    "    Returns:\n",
    "        List of all active dates for the service\n",
    "    \"\"\"\n",
    "    # Filter for the specific service_id\n",
    "    service_row = df[df['service_id'] == service_id]\n",
    "   \n",
    "    if service_row.empty:\n",
    "        print(f\"Service ID '{service_id}' not found\")\n",
    "        return []\n",
    "   \n",
    "    # Get the first (and should be only) row\n",
    "    service = service_row.iloc[0]\n",
    "    start_date = pd.to_datetime(service['start_date'], format='%Y%m%d')\n",
    "    end_date = pd.to_datetime(service['end_date'], format='%Y%m%d')\n",
    "   \n",
    "    # Days of week mapping (Monday=0, Sunday=6)\n",
    "    day_columns = ['monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday']\n",
    "    days_active = [i for i, day_col in enumerate(day_columns) if service[day_col] == 1]\n",
    "   \n",
    "    if not days_active:\n",
    "        print(f\"Service ID '{service_id}' has no active days\")\n",
    "        return []\n",
    "   \n",
    "    # Generate all dates in the range that fall on active days\n",
    "    active_dates = []\n",
    "    current_date = start_date\n",
    "   \n",
    "    while current_date <= end_date:\n",
    "        # Check if current date's weekday is in our active days\n",
    "        if current_date.weekday() in days_active:\n",
    "            active_dates.append(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "   \n",
    "    if to_string:\n",
    "        active_dates = [date.strftime(date_format) for date in active_dates]\n",
    "    return active_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15be67f2",
   "metadata": {},
   "source": [
    "## Update routes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dce1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_services = trips_df[\"service_id\"].unique()\n",
    "trip_dates = {\n",
    "    service: get_active_dates(calendar_df, service)\n",
    "    for service in unique_services\n",
    "}\n",
    "trip_first_date = {\n",
    "    service: dates[0] if dates else None\n",
    "    for service, dates in trip_dates.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aa98218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trips2latest_routes(trips_df, trip_first_date, routes_txt):\n",
    "    extended_trips = trips_df.copy()\n",
    "    extended_trips[\"first_date\"] = extended_trips[\"service_id\"].map(trip_first_date)\n",
    "    extended_trips = extended_trips.copy()\n",
    "    extended_trips = extended_trips[[\"service_id\", \"route_id\", \"shape_id\", \"trip_headsign\", \"direction_id\", \"first_date\"]]\n",
    "    extended_trips = extended_trips.groupby([\"route_id\", \"shape_id\", \"trip_headsign\", \"direction_id\", \"first_date\"]).count().reset_index()\n",
    "    extended_trips = extended_trips.sort_values(by=['route_id', 'direction_id', 'service_id'], ascending=[True, True, False])\n",
    "    extended_trips = extended_trips.drop_duplicates(subset=['route_id', 'direction_id'], ignore_index=True)\n",
    "    extended_trips = extended_trips.rename(columns={\"shape_id\" : \"main_shape_id\", \"first_date\" : \"valid_from\"})\n",
    "    \n",
    "    latest_routes_df = pd.merge(\n",
    "        routes_txt,\n",
    "        extended_trips[[\"route_id\", \"main_shape_id\", \"trip_headsign\", \"direction_id\", \"valid_from\"]],\n",
    "        on=\"route_id\",\n",
    "        how=\"inner\",)\n",
    "    \n",
    "    return latest_routes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08b16528",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_routes_df = trips2latest_routes(trips_df, trip_first_date, routes_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5d691d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_routes_df(routes_df, latest_routes_df):\n",
    "    # Use relevant columns, without route_desc\n",
    "    cols_to_use = [col for col in routes_df.columns]\n",
    "\n",
    "    # Select new rows - rows whats route_id is not in routes_df \n",
    "    new_routes = latest_routes_df[~latest_routes_df[\"route_id\"].isin(routes_df[\"route_id\"])][cols_to_use]\n",
    "    \n",
    "    # Concatenate new routes\n",
    "    updated_routes_df = pd.concat([routes_df, new_routes], ignore_index=True)\n",
    "\n",
    "    # Check for duplicates\n",
    "    duplicates = updated_routes_df[updated_routes_df.groupby(\"route_id\")[\"route_id\"].transform(\"count\") > 2]\n",
    "\n",
    "    if not duplicates.empty:\n",
    "        print(f\"Warning: There are {duplicates['route_id'].nunique()} duplicated route_id(s) in routes_df!\")\n",
    "        print(\"Duplicated route_id(s):\")\n",
    "        print(duplicates['route_id'].unique())\n",
    "    else:\n",
    "        print(\"No duplicate route_id found in routes_df.\")\n",
    "\n",
    "    return updated_routes_df\n",
    "\n",
    "def save_routes(routes_df, data_folder):\n",
    "    _, routes_df_path, _, _, _, _ = define_paths(data_folder=data_folder)\n",
    "    routes_df.to_csv(routes_df_path, index=False)\n",
    "    print(f\"routes_df saved to {routes_df_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02d7da5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate route_id found in routes_df.\n",
      "routes_df saved to ../data/processed/routes.csv\n"
     ]
    }
   ],
   "source": [
    "updated_routes_df = update_routes_df(routes_df, latest_routes_df)\n",
    "save_routes(updated_routes_df, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f27bae",
   "metadata": {},
   "source": [
    "## Update route_versions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "608c415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def version_exists(current_versions, row):\n",
    "    return (\n",
    "        ((current_versions[\"route_id\"] == row[\"route_id\"]) &\n",
    "         (current_versions[\"direction_id\"] == row[\"direction_id\"]) &\n",
    "         (current_versions[\"main_shape_id\"] == row[\"main_shape_id\"]) &\n",
    "         (current_versions[\"trip_headsign\"] == row[\"trip_headsign\"]))\n",
    "        .any()\n",
    "    )\n",
    "\n",
    "def update_route_versions(route_versions_df, latest_routes_df, date):\n",
    "    route_versions_copy_df = route_versions_df.copy()\n",
    "    # version_id starting point\n",
    "    START_VERSION_ID = 100_000\n",
    "\n",
    "    # If the file is empty\n",
    "    if route_versions_df.empty:\n",
    "        next_version_id = START_VERSION_ID\n",
    "    else:\n",
    "        next_version_id = route_versions_df[\"version_id\"].max() + 1\n",
    "\n",
    "    # Create a new versions dataframe\n",
    "    new_versions_df = latest_routes_df.copy()[[\"route_id\", \"main_shape_id\", \"trip_headsign\", \"direction_id\", \"route_desc\", \"valid_from\"]]\n",
    "    new_versions_df[\"valid_from\"] = pd.to_datetime(new_versions_df['valid_from'])\n",
    "    new_versions_df[\"valid_to\"] = pd.NaT\n",
    "    new_versions_df[\"parent_version_id\"] = np.nan\n",
    "    new_versions_df[\"note\"] = np.nan\n",
    "\n",
    "    # Define the current versions\n",
    "    current_versions = route_versions_df[route_versions_df[\"valid_to\"].isna()]\n",
    "\n",
    "    # Let only the new versions\n",
    "    new_versions_filtered = new_versions_df[~new_versions_df.apply(lambda row: version_exists(row, current_versions), axis=1)].copy()\n",
    "\n",
    "    # Update the previous versions valid_to date\n",
    "    for _, row in new_versions_filtered.iterrows():\n",
    "        mask = (\n",
    "            (route_versions_df[\"route_id\"] == row[\"route_id\"]) &\n",
    "            (route_versions_df[\"valid_to\"].isna())\n",
    "        )\n",
    "        route_versions_copy_df.loc[mask, \"valid_to\"] = row[\"valid_from\"] - pd.Timedelta(days=1)\n",
    "\n",
    "    new_versions_filtered[\"version_id\"] = range(next_version_id, next_version_id + len(new_versions_filtered))\n",
    "\n",
    "    # Concat\n",
    "    extended_route_versions_df = pd.concat([route_versions_copy_df, new_versions_filtered], ignore_index=True)\n",
    "\n",
    "    return extended_route_versions_df\n",
    "\n",
    "def save_route_versions(route_versions_df, data_folder):\n",
    "    _, _, route_versions_df_path, _, _, _ = define_paths(data_folder=data_folder)\n",
    "    route_versions_df.to_csv(route_versions_df_path, index=False)\n",
    "    print(f\"routes_df saved to {route_versions_df_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "565297c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "routes_df saved to ../data/processed/route_versions.csv\n"
     ]
    }
   ],
   "source": [
    "extended_route_versions_df = update_route_versions(route_versions_df, latest_routes_df, date)\n",
    "save_route_versions(extended_route_versions_df, data_folder)\n",
    "#extended_route_versions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81d06b",
   "metadata": {},
   "source": [
    "## Update shape_variants_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e992250",
   "metadata": {},
   "source": [
    "### First step update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4284f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_noexception(trip_dates, trips_df):\n",
    "    non_empty_keys = [key for key, value in trip_dates.items() if value]\n",
    "\n",
    "    inservice_df = trips_df[trips_df[\"service_id\"].isin(non_empty_keys)]\n",
    "    inservice_df = inservice_df[[\"service_id\", \"route_id\", \"shape_id\", \"trip_headsign\", \"direction_id\"]]\n",
    "    inservice_df = inservice_df.groupby([\"route_id\", \"shape_id\", \"trip_headsign\", \"direction_id\"]).agg(\"first\").reset_index()\n",
    "\n",
    "    # First add the list column\n",
    "    inservice_df['date_list'] = inservice_df['service_id'].map(trip_dates)\n",
    "\n",
    "    # Then explode the list column into separate rows\n",
    "    df_noexceptions = inservice_df.explode('date_list')\n",
    "    df_noexceptions = df_noexceptions.rename(columns={'date_list': 'date'})\n",
    "    df_noexceptions.drop(columns=['service_id'], inplace=True)\n",
    "    df_noexceptions['exception_type'] = np.nan\n",
    "    \n",
    "    return df_noexceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ac678b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noexceptions = get_df_with_noexception(trip_dates, trips_df)\n",
    "#df_noexceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96700e8d",
   "metadata": {},
   "source": [
    "### Second step update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cfc7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_with_exception(calendar_dates_df, trips_df):\n",
    "    calendar_dates_df['date'] = pd.to_datetime(calendar_dates_df['date'], format=\"%Y%m%d\")\n",
    "\n",
    "    extra_service_ids = calendar_dates_df[[\"date\", \"service_id\", \"exception_type\"]].copy()\n",
    "    extra_service_ids[\"exception_type\"] = extra_service_ids[\"exception_type\"].astype(int)\n",
    "\n",
    "    df_exceptions = pd.merge(trips_df, extra_service_ids, how=\"left\", on=\"service_id\")\n",
    "    df_exceptions = df_exceptions.groupby([\"route_id\", \"shape_id\", \"trip_headsign\", \"direction_id\", \"date\"]).agg('first').reset_index()\n",
    "    df_exceptions = df_exceptions[[\"date\", \"route_id\", \"shape_id\", \"trip_headsign\", \"direction_id\", \"exception_type\"]]\n",
    "\n",
    "    # This will actually convert 2.0 to \"2\" and keep NaN as NaN\n",
    "    df_exceptions[\"exception_type\"] = df_exceptions[\"exception_type\"].apply(\n",
    "        lambda x: str(int(x)) if pd.notna(x) else x\n",
    "    )\n",
    "\n",
    "    return df_exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac77889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exceptions = get_df_with_exception(calendar_dates_df, trips_df)\n",
    "#df_exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ba6bf",
   "metadata": {},
   "source": [
    "### Third step: merge updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8310dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_exception_dfs(df_noexceptions, df_exceptions):\n",
    "    df1 = df_noexceptions.copy()\n",
    "    df2 = df_exceptions.copy()\n",
    "\n",
    "    # Egységes date formátum mindkettőben\n",
    "    df1['date'] = pd.to_datetime(df1['date']).dt.strftime('%Y-%m-%d')\n",
    "    df2['date'] = pd.to_datetime(df2['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Összefűzzük a kettőt\n",
    "    combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Az oszlopok, amelyek alapján duplikációt nézünk (exception_type nélkül)\n",
    "    cols_except_exception = [col for col in combined.columns if col != 'exception_type']\n",
    "\n",
    "    # Megjelöljük, hogy az exception_type NaN-e\n",
    "    combined['__is_nan__'] = combined['exception_type'].isna()\n",
    "\n",
    "    # Rendezés: nem-NaN előre\n",
    "    combined_sorted = combined.sort_values('__is_nan__')\n",
    "\n",
    "    # Duplikátumok kiszűrése\n",
    "    before = len(combined_sorted)\n",
    "    merged_df = combined_sorted.drop_duplicates(subset=cols_except_exception, keep='first')\n",
    "    after = len(merged_df)\n",
    "    removed = before - after\n",
    "\n",
    "    # Segédoszlop eltávolítása\n",
    "    merged_df = merged_df.drop(columns='__is_nan__')\n",
    "\n",
    "    # Rendezés a megadott oszlopok szerint\n",
    "    sort_columns = ['date', 'route_id', 'direction_id', 'shape_id', 'trip_headsign', 'exception_type']\n",
    "    merged_df = merged_df.sort_values(by=sort_columns)\n",
    "\n",
    "    # Index újraszámozása\n",
    "    merged_df = merged_df.reset_index(drop=True)\n",
    "\n",
    "    # Jelentés\n",
    "    print(f\"{removed} duplikált sort eltávolítottunk, ahol csak az exception_type tért el (NaN vs nem-NaN).\")\n",
    "    return merged_df\n",
    "\n",
    "def get_extended4shape_variants(extended_route_versions_df, df_noexceptions, df_exceptions):\n",
    "    valid_routes = extended_route_versions_df[extended_route_versions_df[\"valid_to\"].isna()][[\"version_id\", \"route_id\", \"direction_id\", \"main_shape_id\"]]\n",
    "\n",
    "    merged_df = merge_exception_dfs(df_noexceptions, df_exceptions)\n",
    "    return_df = pd.merge(valid_routes, merged_df, on=[\"route_id\", \"direction_id\"])\n",
    "    return_df[\"main_shape_id\"] = (return_df[\"main_shape_id\"] == return_df[\"shape_id\"]).astype(int)\n",
    "    return_df = return_df.rename(columns={\"main_shape_id\" : \"is_main\"})\n",
    "    return return_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "565faabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7629 duplikált sort eltávolítottunk, ahol csak az exception_type tért el (NaN vs nem-NaN).\n"
     ]
    }
   ],
   "source": [
    "return_df = get_extended4shape_variants(extended_route_versions_df, df_noexceptions, df_exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe6a9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_shape_variants_and_activations(return_df, shape_variants_df, shape_variant_activations_df):\n",
    "    # Get unique shape variants from merged_df\n",
    "    new_variants = return_df[['version_id', 'shape_id', 'trip_headsign', 'is_main']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Check which variants are already in shape_variants_df\n",
    "    if not shape_variants_df.empty:\n",
    "        existing_variants = shape_variants_df[['version_id', 'shape_id', 'trip_headsign', 'is_main']]\n",
    "        # Find variants that don't already exist\n",
    "        merged_check = new_variants.merge(\n",
    "            existing_variants, \n",
    "            on=['version_id', 'shape_id', 'trip_headsign', 'is_main'], \n",
    "            how='left', \n",
    "            indicator=True\n",
    "        )\n",
    "        truly_new_variants = merged_check[merged_check['_merge'] == 'left_only'].drop('_merge', axis=1).reset_index(drop=True)\n",
    "    else:\n",
    "        truly_new_variants = new_variants\n",
    "\n",
    "    # Add new variants to shape_variants_df\n",
    "    if not truly_new_variants.empty:\n",
    "        # Determine starting shape_variant_id\n",
    "        if shape_variants_df.empty:\n",
    "            start_id = 100000\n",
    "        else:\n",
    "            start_id = shape_variants_df['shape_variant_id'].max() + 1\n",
    "        \n",
    "        # Create new variant records\n",
    "        new_variant_records = truly_new_variants.copy()\n",
    "        new_variant_records['shape_variant_id'] = range(start_id, start_id + len(truly_new_variants))\n",
    "        new_variant_records['note'] = None\n",
    "        new_variant_records = new_variant_records[['shape_variant_id', 'version_id', 'shape_id', 'trip_headsign', 'is_main', 'note']]\n",
    "        \n",
    "        # Append to existing shape_variants_df\n",
    "        shape_variants_df = pd.concat([shape_variants_df, new_variant_records], ignore_index=True)\n",
    "\n",
    "    # Create mapping for all variants (existing + new)\n",
    "    variant_mapping = shape_variants_df[['shape_variant_id', 'version_id', 'shape_id', 'trip_headsign', 'is_main']].copy()\n",
    "\n",
    "    # Merge merged_df with variant mapping to get shape_variant_id for each row\n",
    "    merged_with_variant_id = return_df.merge(\n",
    "        variant_mapping, \n",
    "        on=['version_id', 'shape_id', 'trip_headsign', 'is_main'], \n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Create new activation records\n",
    "    new_activations = merged_with_variant_id[['date', 'shape_variant_id', 'exception_type']].copy()\n",
    "    new_activations['exception_type'] = new_activations['exception_type'].astype('float64')\n",
    "\n",
    "    # Check which activations are already in shape_variant_activations_df\n",
    "    if not shape_variant_activations_df.empty:\n",
    "        # Find activations that don't already exist\n",
    "        merged_activations_check = new_activations.merge(\n",
    "            shape_variant_activations_df, \n",
    "            on=['date', 'shape_variant_id', 'exception_type'], \n",
    "            how='left', \n",
    "            indicator=True\n",
    "        )\n",
    "        truly_new_activations = merged_activations_check[merged_activations_check['_merge'] == 'left_only'].drop('_merge', axis=1).reset_index(drop=True)\n",
    "    else:\n",
    "        truly_new_activations = new_activations\n",
    "\n",
    "    # Add new activations to shape_variant_activations_df\n",
    "    if not truly_new_activations.empty:\n",
    "        shape_variant_activations_df = pd.concat([shape_variant_activations_df, truly_new_activations], ignore_index=True)\n",
    "\n",
    "    shape_variant_activations_df.sort_values(['date', 'shape_variant_id'], inplace=True)\n",
    "    shape_variant_activations_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Display results\n",
    "    print(\"Updated shape_variants_df:\")\n",
    "    #print(shape_variants_df.head(10))\n",
    "    print(f\"Shape: {shape_variants_df.shape}\")\n",
    "    print()\n",
    "\n",
    "    print(\"Updated shape_variant_activations_df:\")\n",
    "    #print(shape_variant_activations_df.head(10))\n",
    "    print(f\"Shape: {shape_variant_activations_df.shape}\")\n",
    "\n",
    "    # Summary\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Total unique shape variants: {len(shape_variants_df)}\")\n",
    "    print(f\"Total shape variant activations: {len(shape_variant_activations_df)}\")\n",
    "    if not truly_new_variants.empty:\n",
    "        print(f\"New variants added: {len(truly_new_variants)}\")\n",
    "        print(f\"Shape variant IDs added: {truly_new_variants['shape_variant_id'].min()} - {truly_new_variants['shape_variant_id'].max()}\" if 'shape_variant_id' in locals() else \"\")\n",
    "    else:\n",
    "        print(\"No new variants added\")\n",
    "    if not truly_new_activations.empty:\n",
    "        print(f\"New activations added: {len(truly_new_activations)}\")\n",
    "    else:\n",
    "        print(\"No new activations added\")\n",
    "\n",
    "    return shape_variants_df, shape_variant_activations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d247b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated shape_variants_df:\n",
      "Shape: (1151, 6)\n",
      "\n",
      "Updated shape_variant_activations_df:\n",
      "Shape: (38754, 3)\n",
      "\n",
      "Summary:\n",
      "Total unique shape variants: 1151\n",
      "Total shape variant activations: 38754\n",
      "No new variants added\n",
      "No new activations added\n"
     ]
    }
   ],
   "source": [
    "shape_variants_df, shape_variant_activations_df = update_shape_variants_and_activations(return_df, shape_variants_df, shape_variant_activations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bacb525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_shape_variants_df(shape_variants_df, data_folder):\n",
    "    _, _, _, shape_variants_df_path, _, _ = define_paths(data_folder=data_folder)\n",
    "    shape_variants_df.to_csv(shape_variants_df_path, index=False)\n",
    "    print(f\"shape_variants_df saved to {shape_variants_df_path}\")\n",
    "\n",
    "def save_shape_variant_activations_df(shape_variant_activations_df, data_folder):\n",
    "    _, _, _, _, shape_variant_activations_df_path, _ = define_paths(data_folder=data_folder)\n",
    "    shape_variant_activations_df.to_csv(shape_variant_activations_df_path, index=False)\n",
    "    print(f\"shape_variant_activations_df saved to {shape_variant_activations_df_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "661c267b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape_variants_df saved to ../data/processed/shape_variants.csv\n",
      "shape_variant_activations_df saved to ../data/processed/shape_variant_activations.csv\n"
     ]
    }
   ],
   "source": [
    "save_shape_variants_df(shape_variants_df, data_folder)\n",
    "save_shape_variant_activations_df(shape_variant_activations_df, data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc3cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "budapest_tt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
